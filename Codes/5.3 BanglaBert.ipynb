{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37623,"status":"ok","timestamp":1687521297100,"user":{"displayName":"Ahmadul Karim Chowdhury","userId":"09340342089296209091"},"user_tz":-360},"id":"PZPZh5lih2OW","outputId":"eeda2d17-ad47-48ac-d3c9-f1616b29f88e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","/content/drive/MyDrive/Colab Notebooks/Thesis Project\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","%cd drive/MyDrive/Colab Notebooks/Thesis Project/\n","\n","!pip install transformers\n","import torch\n","import pandas as pd\n","import numpy as np\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","\n","from transformers import BertModel, BertTokenizer\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"]},{"cell_type":"code","source":["print(torch.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SlQotNqRJZbz","executionInfo":{"status":"ok","timestamp":1687521297101,"user_tz":-360,"elapsed":17,"user":{"displayName":"Ahmadul Karim Chowdhury","userId":"09340342089296209091"}},"outputId":"7eb275a1-d6e1-4e9e-8886-b9acee078be5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.0.1+cu118\n"]}]},{"cell_type":"markdown","source":["### Define Hyperparameter"],"metadata":{"id":"Tep-xoAz9Spx"}},{"cell_type":"code","source":["# Define batch size\n","BATCH_SIZE = 32\n","LEARNING_RATE = 0.01\n","\n","DROPOUT = 0.1\n","\n","# Training loop\n","NUM_EPOCHS = 10\n","\n","# Define number of folds for cross-validation\n","NUM_FOLDS = 5\n","\n","# Momentum -> hyperparameter of Stochastic Gradient Descent (SGD) optimizer\n","MOMENTUM = 0.9\n"],"metadata":{"id":"juxFG72o75lG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the data\n","df = pd.read_excel('0. All Datasets.xlsx')\n","\n","df['label'] = df['label'].map({'Non Depressive': 0, 'Depressive': 1})\n","# Drop rows with any missing values\n","df = df.dropna(subset=['text', 'label'])\n","# drop rows that have identical values in all columns\n","df.drop_duplicates(inplace=True)\n","\n","# Assuming your DataFrame is named 'df'\n","columns_to_drop = [col for col in df.columns if col not in ['text', 'label']]\n","df.drop(columns=columns_to_drop, inplace=True)\n","\n","print(df.info())\n","\n","label_counts = df['label'].value_counts()\n","print(\"Label Details:\")\n","print(label_counts)\n","\n","df.sample(n=10)  # Change the value of 'n' to the desired number of random samples"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":621},"id":"erIPs2Khoevg","executionInfo":{"status":"ok","timestamp":1687521303709,"user_tz":-360,"elapsed":6624,"user":{"displayName":"Ahmadul Karim Chowdhury","userId":"09340342089296209091"}},"outputId":"ea79ca37-4282-4699-953f-235302d53ce9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 31695 entries, 0 to 31698\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   text    31695 non-null  object\n"," 1   label   31695 non-null  int64 \n","dtypes: int64(1), object(1)\n","memory usage: 742.9+ KB\n","None\n","Label Details:\n","0    16719\n","1    14976\n","Name: label, dtype: int64\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                    text  label\n","16814  আমার পছন্দের মানুষ আমার সেরা বন্ধুকে পছন্দ করে...      0\n","3571   আমি আপনার সাহায্য এবং পরামর্শ চাই, অনুগ্রহ করে...      0\n","5825   আমার বয়স ১৫(এম ), জীবন সবে শুরু হয়েছে। এবং আ...      1\n","27451  আমি মোটেও এই মুভিটি দেখার অনুরাগী ছিলাম না, কি...      0\n","25278  এটি একটি উদ্ভট হিস্ট/ক্যাপার ফিল্ম, যা প্রথমে ...      0\n","2279   তাই আমার এই বন্ধুর মনে হয় অ্যানহেডোনিয়ার কাছ...      1\n","3754   আমি ট্রান্স (এমটিএফ) আমি অনেক বছর ধরে চিন্তা ক...      0\n","19753  আমার সঙ্গী হল উপার্জনকারী এবং আমি যা করতে পারি...      1\n","10818  কোন ছন্দ বা কারণ নেই, আমরা এমনকি একে অপরকে কিছ...      1\n","10995  ছেলেদের যখনই দরকার হবে তাদের জন্য উপহার.... „ ...      0"],"text/html":["\n","  <div id=\"df-cc3202bf-4451-4fd3-a399-e1d9b19c64a7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>16814</th>\n","      <td>আমার পছন্দের মানুষ আমার সেরা বন্ধুকে পছন্দ করে...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3571</th>\n","      <td>আমি আপনার সাহায্য এবং পরামর্শ চাই, অনুগ্রহ করে...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5825</th>\n","      <td>আমার বয়স ১৫(এম ), জীবন সবে শুরু হয়েছে। এবং আ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>27451</th>\n","      <td>আমি মোটেও এই মুভিটি দেখার অনুরাগী ছিলাম না, কি...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25278</th>\n","      <td>এটি একটি উদ্ভট হিস্ট/ক্যাপার ফিল্ম, যা প্রথমে ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2279</th>\n","      <td>তাই আমার এই বন্ধুর মনে হয় অ্যানহেডোনিয়ার কাছ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3754</th>\n","      <td>আমি ট্রান্স (এমটিএফ) আমি অনেক বছর ধরে চিন্তা ক...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19753</th>\n","      <td>আমার সঙ্গী হল উপার্জনকারী এবং আমি যা করতে পারি...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10818</th>\n","      <td>কোন ছন্দ বা কারণ নেই, আমরা এমনকি একে অপরকে কিছ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10995</th>\n","      <td>ছেলেদের যখনই দরকার হবে তাদের জন্য উপহার.... „ ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc3202bf-4451-4fd3-a399-e1d9b19c64a7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cc3202bf-4451-4fd3-a399-e1d9b19c64a7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cc3202bf-4451-4fd3-a399-e1d9b19c64a7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Sample 14,000 instances for each label\n","df_label_0 = df[df['label'] == 0].sample(n=14000)\n","df_label_1 = df[df['label'] == 1].sample(n=14000)\n","\n","# Concatenate the sampled dataframes\n","df = pd.concat([df_label_0, df_label_1], ignore_index=True)\n","\n","print(df.info())\n","\n","label_counts = df['label'].value_counts()\n","print(\"Label Details:\")\n","print(label_counts)\n","\n","df.sample(n=10)  # Change the value of 'n' to the desired number of random samples"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":621},"id":"brJpM3Cap_ze","executionInfo":{"status":"ok","timestamp":1687521303711,"user_tz":-360,"elapsed":30,"user":{"displayName":"Ahmadul Karim Chowdhury","userId":"09340342089296209091"}},"outputId":"73537f20-22ae-40f3-9786-cfa248b571e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 28000 entries, 0 to 27999\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   text    28000 non-null  object\n"," 1   label   28000 non-null  int64 \n","dtypes: int64(1), object(1)\n","memory usage: 437.6+ KB\n","None\n","Label Details:\n","0    14000\n","1    14000\n","Name: label, dtype: int64\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                    text  label\n","1277   কার ওয়াই ওয়াং এর অবিশ্বাস্যভাবে চিত্তাকর্ষক ...      0\n","2696   অনুমান আমার পরামর্শ দরকার, এটা খুবই খারাপ আমার...      0\n","8566   আপনি কি কখনো সেই অনুভূতি পান? আমি প্রথম লকডাউন...      0\n","21408  আমি অনেক আগেই আমার শেষের পরিকল্পনা করা বন্ধ কর...      1\n","15336  // দুঃখিত যদি এই পুরো পোস্টটি একটি জগাখিচুড়ি ...      1\n","16284  আমার উদ্বেগের কারণে (এবং বিষণ্নতা?) আমি ক্রমাগ...      1\n","4992   সে ইজ দ্য ম্যান ছিল যা আমি চেয়েছিলাম এবং হয়ত...      0\n","6957   আমার গ্রেট ঠাকুমা একজন বদমাশ ছিলেন! আরে কিশোরর...      0\n","21682  আমার ভাই [M24] বিয়ে করছে এবং প্রতিদিন সে এবং ...      1\n","7056   রাজনৈতিক পোস্ট। ট্রাম্পপ্রেমীরা দূরে থাকুন! আপ...      0"],"text/html":["\n","  <div id=\"df-9851c9e7-f885-4baa-b3b2-e3b95841931d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1277</th>\n","      <td>কার ওয়াই ওয়াং এর অবিশ্বাস্যভাবে চিত্তাকর্ষক ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2696</th>\n","      <td>অনুমান আমার পরামর্শ দরকার, এটা খুবই খারাপ আমার...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8566</th>\n","      <td>আপনি কি কখনো সেই অনুভূতি পান? আমি প্রথম লকডাউন...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21408</th>\n","      <td>আমি অনেক আগেই আমার শেষের পরিকল্পনা করা বন্ধ কর...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>15336</th>\n","      <td>// দুঃখিত যদি এই পুরো পোস্টটি একটি জগাখিচুড়ি ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16284</th>\n","      <td>আমার উদ্বেগের কারণে (এবং বিষণ্নতা?) আমি ক্রমাগ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4992</th>\n","      <td>সে ইজ দ্য ম্যান ছিল যা আমি চেয়েছিলাম এবং হয়ত...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6957</th>\n","      <td>আমার গ্রেট ঠাকুমা একজন বদমাশ ছিলেন! আরে কিশোরর...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21682</th>\n","      <td>আমার ভাই [M24] বিয়ে করছে এবং প্রতিদিন সে এবং ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7056</th>\n","      <td>রাজনৈতিক পোস্ট। ট্রাম্পপ্রেমীরা দূরে থাকুন! আপ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9851c9e7-f885-4baa-b3b2-e3b95841931d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9851c9e7-f885-4baa-b3b2-e3b95841931d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9851c9e7-f885-4baa-b3b2-e3b95841931d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["X = df['text']\n","y = df['label']\n","print(X.info())\n","print(y.info())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mTmJIuExpeY3","executionInfo":{"status":"ok","timestamp":1687521303711,"user_tz":-360,"elapsed":20,"user":{"displayName":"Ahmadul Karim Chowdhury","userId":"09340342089296209091"}},"outputId":"982832d3-bb0c-4aef-9eb7-80e52daba1c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.series.Series'>\n","RangeIndex: 28000 entries, 0 to 27999\n","Series name: text\n","Non-Null Count  Dtype \n","--------------  ----- \n","28000 non-null  object\n","dtypes: object(1)\n","memory usage: 218.9+ KB\n","None\n","<class 'pandas.core.series.Series'>\n","RangeIndex: 28000 entries, 0 to 27999\n","Series name: label\n","Non-Null Count  Dtype\n","--------------  -----\n","28000 non-null  int64\n","dtypes: int64(1)\n","memory usage: 218.9 KB\n","None\n"]}]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"BekcQiBrJVY1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define the BERT model architecture"],"metadata":{"id":"2seNRUyUTH-s"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":183639,"status":"ok","timestamp":1687521776657,"user":{"displayName":"Ahmadul Karim Chowdhury","userId":"09340342089296209091"},"user_tz":-360},"id":"lRjZuXCFh6dy","outputId":"649fb98f-f0b1-4de7-87a4-09a00f7fb9e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing Fold: 1\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-12-946cccec6c68>:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  torch.tensor(y_train_fold))\n","<ipython-input-12-946cccec6c68>:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  torch.tensor(y_val_fold))\n"]},{"output_type":"stream","name":"stdout","text":["Processing Fold: 2\n","Processing Fold: 3\n","Processing Fold: 4\n","Processing Fold: 5\n"]}],"source":["# Define the BERT model architecture\n","class BERTClassifier(nn.Module):\n","    def __init__(self, bert_model, hidden_dim, num_classes):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert_model\n","        self.dropout = nn.Dropout(DROPOUT)\n","        self.fc = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = output['pooler_output']\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.fc(pooled_output)\n","        return torch.sigmoid(logits)\n","\n","\n","# Set device\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Load the BERT tokenizer and encode the text data\n","tokenizer = BertTokenizer.from_pretrained('sagorsarker/bangla-bert-base')\n","X_encodings = tokenizer(list(X), truncation=True, padding=True, max_length=512)\n","\n","# Convert labels to tensor\n","y_tensor = torch.tensor(y.values)\n","\n","# Initialize the StratifiedKFold\n","skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n","\n","# Perform cross-validation\n","for fold, (train_index, val_index) in enumerate(skf.split(X_encodings['input_ids'], y_tensor), 1):\n","    print(f\"Processing Fold: {fold}\")\n","\n","    # Convert X_encodings to a list of tuples\n","    X_encodings_list = list(X_encodings.items())\n","\n","    # Split data into train and validation sets\n","    X_train_fold = {key: [value[i] for i in train_index] for key, value in X_encodings_list}\n","    X_val_fold = {key: [value[i] for i in val_index] for key, value in X_encodings_list}\n","    y_train_fold = y_tensor[train_index]\n","    y_val_fold = y_tensor[val_index]\n","\n","\n","    # Create PyTorch datasets\n","    train_dataset = TensorDataset(torch.tensor(X_train_fold['input_ids']),\n","                                  torch.tensor(X_train_fold['attention_mask']),\n","                                  torch.tensor(y_train_fold))\n","    val_dataset = TensorDataset(torch.tensor(X_val_fold['input_ids']),\n","                                torch.tensor(X_val_fold['attention_mask']),\n","                                torch.tensor(y_val_fold))\n","\n","    # Create data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n"]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n"],"metadata":{"id":"_p7TNgcbJ37z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the BERT classifier model\n","class BERTClassifier(nn.Module):\n","    def __init__(self, bert_model, hidden_dim, num_classes):\n","        super(BERTClassifier, self).__init__()\n","        self.bert_model = bert_model\n","        self.fc = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        output = self.bert_model(input_ids, attention_mask)[1]  # Use the [CLS] token representation\n","        logits = self.fc(output)\n","        return logits\n","\n","# Set the device\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize the BERT model\n","bert_model = BertModel.from_pretrained('sagorsarker/bangla-bert-base')\n","\n","\n","hidden_dim = bert_model.config.hidden_size\n","num_classes = 2\n","\n","model = BERTClassifier(bert_model, hidden_dim, num_classes).to(DEVICE)\n","\n","\n","# Define the optimizer and loss function\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n","criterion = nn.CrossEntropyLoss()\n","\n","best_accuracy = 0.0\n","\n","# Freeze BERT layer\n","for param in model.bert_model.parameters():\n","    param.requires_grad = False\n","\n","# Unfreeze output layer\n","for param in model.fc.parameters():\n","    param.requires_grad = True\n","\n","# Define lists to store metrics for each fold\n","fold_train_losses = []\n","fold_train_accuracies = []\n","fold_val_losses = []\n","fold_val_accuracies = []\n","fold_val_predictions = []\n","fold_val_labels = []\n","\n","# Define lists to store train and validation metrics per epoch\n","train_losses_per_epoch = []\n","train_accuracies_per_epoch = []\n","val_losses_per_epoch = []\n","val_accuracies_per_epoch = []\n","\n","for epoch in range(NUM_EPOCHS):\n","    model.train()\n","    train_loss = 0.0\n","    train_correct = 0\n","    train_total = 0\n","\n","    for batch in train_loader:\n","      input_ids, attention_mask, labels = batch\n","      input_ids = input_ids.to(DEVICE)\n","      attention_mask = attention_mask.to(DEVICE)\n","      labels = labels.to(DEVICE)\n","\n","      optimizer.zero_grad()\n","\n","      logits = model(input_ids, attention_mask)\n","      _, predicted = torch.max(logits, 1)\n","\n","      loss = criterion(logits, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","      train_loss += loss.item() * input_ids.size(0)\n","      train_total += labels.size(0)\n","      train_correct += (predicted == labels).sum().item()\n","\n","    train_accuracy = 100 * train_correct / train_total\n","\n","    model.eval()\n","    val_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","\n","    with torch.no_grad():\n","        for batch in val_loader:\n","          input_ids, attention_mask, labels = batch\n","          input_ids = input_ids.to(DEVICE)\n","          attention_mask = attention_mask.to(DEVICE)\n","          labels = labels.to(DEVICE)\n","\n","          logits = model(input_ids, attention_mask)\n","          _, predicted = torch.max(logits, 1)\n","\n","          loss = criterion(logits, labels)\n","\n","          val_loss += loss.item() * input_ids.size(0)\n","          val_total += labels.size(0)\n","          val_correct += (predicted == labels).sum().item()\n","\n","          fold_val_predictions.extend(predicted.cpu().numpy())\n","          fold_val_labels.extend(labels.cpu().numpy())\n","\n","    val_accuracy = 100 * val_correct / val_total\n","\n","    fold_train_losses.append(train_loss / train_total)\n","    fold_train_accuracies.append(train_accuracy)\n","    fold_val_losses.append(val_loss / val_total)\n","    fold_val_accuracies.append(val_accuracy)\n","\n","    train_losses_per_epoch.append(train_loss / train_total)\n","    train_accuracies_per_epoch.append(train_accuracy)\n","    val_losses_per_epoch.append(val_loss / val_total)\n","    val_accuracies_per_epoch.append(val_accuracy)\n","\n","    print('Epoch: {}/{} | Train Loss: {:.4f} | Train Accuracy: {:.2f}% | Val Loss: {:.4f} | Val Accuracy: {:.2f}%'.format(\n","        epoch + 1, NUM_EPOCHS, train_loss / train_total, train_accuracy, val_loss / val_total, val_accuracy))\n","\n","    if val_accuracy > best_accuracy:\n","        best_accuracy = val_accuracy\n","        torch.save(model.state_dict(), 'best_model.pth')\n","\n","# Calculate performance metrics\n","average_accuracy = accuracy_score(fold_val_labels, fold_val_predictions)\n","average_precision = precision_score(fold_val_labels, fold_val_predictions)\n","average_recall = recall_score(fold_val_labels, fold_val_predictions)\n","average_f1_score = f1_score(fold_val_labels, fold_val_predictions)\n","\n","# Print average metrics across folds\n","print('Average Metrics Across Folds:')\n","print('Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f} | F1 Score: {:.4f}'.format(\n","    average_accuracy, average_precision, average_recall, average_f1_score))\n","\n","# Plotting the train and validation accuracy and loss graphs\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(train_losses_per_epoch, label='Train Loss')\n","plt.plot(val_losses_per_epoch, label='Val Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(train_accuracies_per_epoch, label='Train Accuracy')\n","plt.plot(val_accuracies_per_epoch, label='Val Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j7Dk24g4oDiX","outputId":"b82e915e-f046-4d0b-9e9a-88c72c4edbeb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at sagorsarker/bangla-bert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1/10 | Train Loss: 0.4214 | Train Accuracy: 80.44% | Val Loss: 0.3309 | Val Accuracy: 85.32%\n","Epoch: 2/10 | Train Loss: 0.3892 | Train Accuracy: 82.84% | Val Loss: 0.3057 | Val Accuracy: 86.93%\n","Epoch: 3/10 | Train Loss: 0.3773 | Train Accuracy: 83.46% | Val Loss: 0.3114 | Val Accuracy: 86.71%\n","Epoch: 4/10 | Train Loss: 0.3874 | Train Accuracy: 83.17% | Val Loss: 0.3881 | Val Accuracy: 83.55%\n","Epoch: 5/10 | Train Loss: 0.3754 | Train Accuracy: 83.86% | Val Loss: 0.4295 | Val Accuracy: 81.16%\n","Epoch: 6/10 | Train Loss: 0.3698 | Train Accuracy: 83.76% | Val Loss: 0.3062 | Val Accuracy: 86.86%\n","Epoch: 7/10 | Train Loss: 0.3839 | Train Accuracy: 83.18% | Val Loss: 0.3037 | Val Accuracy: 87.32%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hHRksR55RNbn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print average metrics across folds\n","print('Average Metrics Across Folds:')\n","print('Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f} | F1 Score: {:.4f}'.format(\n","    average_accuracy, average_precision, average_recall, average_f1_score))"],"metadata":{"id":"wJLkPX-AoDe_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bBd5j3oDk_F5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import StratifiedKFold, GridSearchCV\n","from transformers import BertTokenizer, BertModel\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the data\n","df = pd.read_excel('0. All Datasets.xlsx')\n","df['label'] = df['label'].map({'Non Depressive': 0, 'Depressive': 1})\n","df = df.dropna(subset=['text', 'label'])\n","df.drop_duplicates(inplace=True)\n","columns_to_drop = [col for col in df.columns if col not in ['text', 'label']]\n","df.drop(columns=columns_to_drop, inplace=True)\n","\n","label_counts = df['label'].value_counts()\n","print(\"Label Details:\")\n","print(label_counts)\n","\n","# Sample 14,000 instances for each label\n","df_label_0 = df[df['label'] == 0].sample(n=14000, random_state=42)\n","df_label_1 = df[df['label'] == 1].sample(n=14000, random_state=42)\n","\n","# Concatenate the sampled dataframes\n","df = pd.concat([df_label_0, df_label_1], ignore_index=True)\n","\n","X = df['text']\n","y = df['label']"],"metadata":{"id":"CosnN8h0k_Cx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the BERT classifier model\n","class BERTClassifierWrapper(nn.Module):\n","    def __init__(self, bert_model, hidden_dim, num_classes):\n","        super(BERTClassifierWrapper, self).__init__()\n","        self.bert_model = bert_model\n","        self.fc = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        output = self.bert_model(input_ids, attention_mask)[1]  # Use the [CLS] token representation\n","        logits = self.fc(output)\n","        return logits\n","\n","    def fit(self, input_ids, attention_mask, labels, criterion, optimizer, num_epochs, batch_size):\n","        dataset = TensorDataset(input_ids, attention_mask, labels)\n","        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","        self.train()\n","        for epoch in range(num_epochs):\n","            running_loss = 0.0\n","            for i, batch in enumerate(data_loader):\n","                input_ids_batch, attention_mask_batch, labels_batch = batch\n","                input_ids_batch = input_ids_batch.to(DEVICE)\n","                attention_mask_batch = attention_mask_batch.to(DEVICE)\n","                labels_batch = labels_batch.to(DEVICE)\n","\n","                optimizer.zero_grad()\n","\n","                outputs = self(input_ids_batch, attention_mask_batch)\n","                loss = criterion(outputs, labels_batch)\n","\n","                loss.backward()\n","                optimizer.step()\n","\n","                running_loss += loss.item()\n","\n","            print(f\"Epoch {epoch+1}/{num_epochs} Loss: {running_loss / len(data_loader)}\")\n","\n","    def predict(self, input_ids, attention_mask):\n","        dataset = TensorDataset(input_ids, attention_mask)\n","        data_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n","\n","        self.eval()\n","        predictions = []\n","        with torch.no_grad():\n","            for batch in data_loader:\n","                input_ids_batch, attention_mask_batch = batch\n","                input_ids_batch = input_ids_batch.to(DEVICE)\n","                attention_mask_batch = attention_mask_batch.to(DEVICE)\n","\n","                outputs = self(input_ids_batch, attention_mask_batch)\n","                _, predicted_labels = torch.max(outputs, dim=1)\n","\n","                predictions.extend(predicted_labels.cpu().numpy())\n","\n","        return predictions\n","\n","# Load the BERT tokenizer and encode the text data\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","X_encodings = tokenizer(list(X), truncation=True, padding=True, max_length=512)\n","\n","# Convert labels to tensor\n","y_tensor = torch.tensor(y.values)\n","\n","# Set device\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Define hyperparameters and their possible values for grid search\n","parameters = {\n","    'num_epochs': [5, 10],\n","    'batch_size': [32, 64],\n","    'learning_rate': [0.001, 0.01],\n","    'momentum': [0.9, 0.95],\n","    'dropout': [0.1, 0.2],\n","}\n","\n","# Initialize the BERT model\n","bert_model = BertModel.from_pretrained('bert-base-multilingual-cased')\n","hidden_dim = bert_model.config.hidden_size\n","num_classes = 2\n","\n","# Create the BERT classifier model\n","model = BERTClassifierWrapper(bert_model, hidden_dim, num_classes).to(DEVICE)\n","\n","# Define the optimizer and loss function\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Define the grid search\n","grid_search = GridSearchCV(model, parameters, cv=5, scoring='accuracy')\n","\n","# Run grid search\n","grid_search.fit(X_encodings['input_ids'], X_encodings['attention_mask'], y_tensor, criterion, optimizer)\n","\n","# Print the best parameters and score\n","print(\"Best Parameters: \", grid_search.best_params_)\n","print(\"Best Score: \", grid_search.best_score_)"],"metadata":{"id":"QEFFs_cYk-_6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"65Wt5dEck-4Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YM_zR66fk-1Z"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1PdjHcPtvADoN-vrPUqdfYrOSWiQd0FTN","timestamp":1686846778720}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}